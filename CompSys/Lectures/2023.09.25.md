# Lecture notes

## Memory hierarchy and caching

### Locality
Programs tend to access data located near that which was accessed recently.
![[Pasted image 20230925131659.png]]

This is quicker since data is stored "close" to each other.

We usually have good locality when writing proper code.

Executes instructions in sequence = *Spatial locality*.
Cycles through loop repeatedly = *Temporal locality*.

### C array layout
C uses *row major order* to represent arrays.

![[Pasted image 20230925132236.png]]

```C
int sumrows(int A[M][N]) {
	int sum = 0;

	for (int i = 0; i < M; i++)
		for (int j = 0; j < N; j++)
			sum += A[i][j];
		return sum;
}
```

This is an example of *good locality*, since i and j are incremented such that values are placed next to each other in an array.

```C
int sumcols(int A[M][N]) {
	int sum = 0;
	for (int j = 0; j < N; j++)
		for (int i = 0; i < M; i++)
			sum += A[i][j];
	return sum;
}
```

This is *bad locality* since arrays in C are stored in *row major order*.

### Memory hierarchies
*Fast storage* is expensive and thus smaller than *slow storage*.
![[Pasted image 20230925132959.png]]

We want to minimise the bottleneck between memory and the CPU (*von Neumann bottleneck*).

The smaller and faster device at level $k$ acts as a cache for the larger slower device at level $k + 1$.

### Caches
General structure of cache:
![[Pasted image 20230925134812.png]]
There is a bit for a line which indicates whether the line is valid or not.

To see CPU info
```bash
sudo dmidecode -t cache
```

#### Address structure
When $S=2^n, B=2^m$ we can easily split a $w$-bit address into fields, writing $x_i$ for bit $i$.
$$
\underbrace{x_{w-1} \cdots x_{m+n+1}}_{\text {tag }} \underbrace{x_{m+n} \cdots x_m}_{\text {set index }} \underbrace{x_{m-1} \cdots x_0}_{\text {block offset }}
$$
Consider an 8-bit address with $m=2, s=3$.
$$
\underbrace{x_7 x_6 x_5}_{\text {tag }} \underbrace{x_4 x_3 x_2}_{\text {tag }} \underbrace{x_1 x_0}_{\text {offset }}
$$
![[Pasted image 20230925135307.png]]


#### Cache hit/miss

**Cold/compulsory miss**:
- Occur when the cache is empty.
- Unavoidable when a program first starts.

**Conflict miss**:
- Most caches limit blocks at level $k + 1$ to a small subset of the slots at level $k$.
- Causes conflicts when cache is large enough, but the blocks being accessed all map to the same slot.

**Capacity miss**:
- Occurs when program *working set* exceeds size of cache.

Conflict miss and capacity miss are related.

# Exercises
[COD] p. 499(677) 
Assume memory is byte addressable and words are 32 bits
## 5.1
In this exercise we look at memory locality properties of matrix computation.  The following code is written in C, where elements within the same row are stored contiguously. Assume each word is a 32-bit integer.

```C
for (I=0; I<8; I++)
  for (J=0; J<8000; J++)
	A[I][J]=B[I][0]+A[J][I];
```

### 1
**How many 32-bit integers can be stored in a 16-byte cache block?**

A 16 byte cache block can keep 4 32 bit integers, since each integer takes up 4 bytes. $\frac{16}{4}=4$

### 2
**Which variable references exhibit temporal locality?**

### 3
**Which references exhibit spatial locality?**
	The inner loop of our algorithm iterates through every $\texttt{j}$, our columns, while the outer loop $\texttt{i}$ iterates through our rows. In *RMO*, each subsequent elements in rows are stored sequentially. When we access these elements in the inner loop, we are 


### 

Locality is affected by both the reference order and data layout. The same computation can also be written below in Matlab, which differs from C in that it stores matrix elements within the same column contiguously in memory.
```C
for I=1:8
	for J=1:8000
		A(I,J)=B(I,0)+A(J,I);
	end
end
```
### 4
### 5