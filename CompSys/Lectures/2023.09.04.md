# Reading notes
[COD] 1.1-1.4, 1.6-1.8
[JG] 1 - 3
## Classes of computing

### Personal computers (PCs)
- Deliver good performance to single users
- Low-cost
- Usually execute third-party software

### Servers
- Usually only accessed via a network
- Usually large computers (can also be a small desktop computer)
- Oriented for carrying sizeable workloads
- Often customised for a particular function
- Greater storage, computing, and I/O than desktop computers

#### Supercomputers
- Consist of hundreds of thousands of processors and terabytes of memory
- Very expensive
- Used for high-end scientific and engineering calculations

### Embedded computers
- Span a wide range of applications and performance
- Can be found in cars, TVs, aeroplanes etc.
- Used in Internet of Things (IoT)
- Largest class of computers
- Embedded applications have unique application requirements that combine a minimum performance with stringent limitations on cost or power
- Low tolerance to failure

![[Pasted image 20230904083105.png]]

### Post PC-era
*Personal mobile devices (PMDs)* are replacing PCs.
![[Pasted image 20230904083550.png]]
Conventional servers are being replaced by *cloud computing*, which relies upon giant data-centres that are now known as *Warehouse Scale Computers (WSCs)*. Portions of these WSCs are rented to provide software services to PMDs without having to build WSCs of their own. This is known as *Software as a Service (SaaS)* via the cloud.

## Introduction to COD
You will learn the following:

- *How are programs written in a high-level language, such as $\mathrm{C}$ or Java, translated into the language of the hardware, and how does the hardware execute the resulting program?* Comprehending these concepts forms the basis of understanding the aspects of both the hardware and software that affect program performance.

- *What is the interface between the software and the hardware, and how does software instruct the hardware to perform needed functions?* These concepts are vital to understanding how to write many kinds of software.

- *What determines the performance of a program, and how can a programmer improve the performance?* As we will see, this depends on the original program, the software translation of that program into the computer's language, and the effectiveness of the hardware in executing the program.

- *What techniques can be used by hardware designers to improve performance?* This book will introduce the basic concepts of modern computer design. The interested reader will find much more material on this topic in our advanced book, Computer Architecture: A Quantitative Approach.

- *What techniques can be used by hardware designers to improve energy efficiency?* What can the programmer do to help or hinder energy efficiency?

- *What are the reasons for and the consequences of the switch from sequential processing to parallel processing?* This book gives the motivation, describes the current hardware mechanisms to support parallelism, and surveys the new generation of "multi-core" microprocessors (see Chapter 6).

- *Since the first commercial computer in 1951, what great ideas did computer architects come up with that lay the foundation of modern computing?*

## Seven Great Ideas

### Use abstraction to simplify design
![[Pasted image 20230904084810.png]]
A major productivity technique for hardware and software is to use abstractions to characterise the design at different levels of representation; lower-level details are hidden to offer a simpler model at higher levels. We’ll use the abstract painting icon to represent this second great idea.

### Make the common case fast
![[Pasted image 20230904084857.png]]
Making the common case fast will tend to enhance performance better than 
optimising the rare case. We use a sports car as the icon for making the common case fast, as the most common trip has one or two passengers, and it’s surely easier to make a fast sports car than a fast minivan!

### Performance via parallelism
![[Pasted image 20230904084954.png]]
Since the dawn of computing, computer architects have offered designs that get 
more performance by computing operations in parallel. We use multiple jet engines of a plane as our icon for 
parallel performance.

### Performance via Pipelining
![[Pasted image 20230904085100.png]]
A particular pattern of parallelism is so prevalent in computer architecture that 
it merits its own name: *pipelining*. For example, before fire engines, a “bucket 
brigade” would respond to a fire, which many cowboy movies show in response to 
a dastardly act by the villain. The townsfolk form a human chain to carry a water 
source to fire, as they could much more quickly move buckets up the chain instead 
of individuals running back and forth. Our pipeline icon is a sequence of pipes, 
with each section representing one stage of the pipeline.

### Performance via Prediction
![[Pasted image 20230904085135.png]]In some cases, it can be faster on average to guess and  start working rather than wait until you know for sure, assuming that the mechanism to recover from a misprediction is not too expensive and your prediction is relatively accurate. We use the fortune-teller’s crystal ball as our prediction icon.

### Hierarchy of Memories
![[Pasted image 20230904085224.png]]
Programmers want the memory to be fast, large, and cheap, as memory speed often 
shapes performance, capacity limits the size of problems that can be solved, and the 
cost of memory today is often the majority of computer cost. Architects have found 
that they can address these conflicting demands with a hierarchy of memories, 
with the fastest, smallest, and the most expensive memory per bit at the top of the 
hierarchy and the slowest, largest, and cheapest per bit at the bottom. Caches give the programmer the illusion that main memory is almost as fast as the top of the hierarchy and nearly as big and cheap as the bottom of the hierarchy. We use a layered triangle icon to represent the memory hierarchy. The shape indicates speed, cost, and size: the closer to the top, the faster and more expensive per bit the memory; the wider the base of the layer, the bigger the memory.

### Dependability via Redundancy
![[Pasted image 20230904085541.png]]
Computers not only need to be fast; they need to be dependable. Since any physical 
device can fail, we make systems dependable by including redundant components that can take over when a failure occurs and to help detect failures. We use the tractor-trailer as our icon, since the dual tires on each side of its rear axles allow the truck to continue driving even when one tire fails. (Presumably, the truck driver heads immediately to a repair facility so the flat tire can be fixed, thereby restoring redundancy!).

## Below Your Program
To go from a complex application to the primitive instructions involves several 
layers of software that interpret or translate high-level operations into simple 
computer instructions, an example of the great idea of *abstraction*.
![[Pasted image 20230904085833.png]]
An *operating system* interfaces between a user's program and the hardware and provides a variety of services and supervisory functions, including:
- Handling basic I/O operations
- Allocating storage and memory
- Providing for protected sharing of the computer among multiple applications  using it simultaneously

### From high-level to hardware
*Compilers* translate high-level languages into assembly language statements.

*Instructions* are collections of bits that the computer is given that perform certain *commands*. These instructions are fundamentally *binary digits* or *bits*, which is a binary system of letters, namely 0 and 1, representing *off* and *on* respectively. *Assemblers* were designed to translate symbolic notation to binary. For example, the assembler can translate this notation
```
add A, b
```
into
```
1001010100101110
```
This language is still used today and is called *assembly language*, while the binary language that the machine understand is the *machine language*.
![[Pasted image 20230904090901.png]]
A compiler enables a programmer to write this high-level language expression:
```
A + B
```
The *compiler* would compile it into this *assembly language* statement:
```
add A, B
```

## Under the covers
### The five classic components of a computer
Input, output, memory, datapath, and control are the five classic components of a computer. 
![[Pasted image 20230904091205.png]]

### Through the looking glass
The most fascinating I/O device is probably the *graphics display*. A typical LCD includes rod-shaped molecules in a liquid that form a twisting  helix that bends light entering the display. The rods straighten out when a current is  applied and no longer bend the light. Since the liquid crystal material is between  two screens polarised at 90°, the light cannot pass through unless it is bent.  Today, most LCDs use an active matrix that has a tiny transistor switch at each pixel to control current precisely and make sharper images. An *RGB* mask associated with each dot on the display determines the intensity of the three-colour components in the final image; in a colour active matrix LCD, there are three transistor switches at each point. The image is composed of a matrix of picture elements, or *pixels*, which can be represented as a matrix of bits, called a *bit map*.

The computer hardware support for graphics consists mainly of a raster refresh 
buffer, or *frame buffer*, to store the *bit map*. The image to be represented onscreen 
is stored in the frame buffer, and the bit pattern per pixel is read out to the graphics 
display at the refresh rate
![[Pasted image 20230904092225.png]]
Each coordinate in the frame buffer on the left determines the shade of the corresponding coordinate for the raster scan CRT display on the right. Pixel ($X_0$, $Y_0$) contains the bit pattern $\texttt{0011}$, which is a lighter shade on the screen than the bit pattern $\texttt{1101}$ in pixel ($X_1$, $Y_1$).

### Opening the box
The devices that drive our advancing  technology are called *integrated circuits*, nicknamed *chips*. The *processor* is the active part of the computer, following the instructions of a program to the letter. It adds numbers, tests numbers, signals I/O devices to activate, and so on. The processor is often called the *central processing unit* or *CPU*.

The processor comprises two main components: *datapath* and *control*. Datapath performs the arithmetic operations and control tells the datapath, memory, and I/O devices what to do according to the wished of the instructions of the program.

Computers also include a *memory chip*. The *memory* is where the programs are kept when they are running along with the data needed by running  programs. The memory is a *DRAM* or *dynamic random-access memory* chip. DRAMS are used together to contain the instructions and data of a program.

Inside the processor there is also *cache memory*, which consists of small, fast memory that acts as a DRAM buffer. Cache is built using different memory technology.*SRAM* or *static random-access memory* is faster but less dense and thus more expensive than DRAM. SRAM and DRAM are two layers of the *memory hierarchy*. 

*RAM* is called*volatile memory* or *primary memory*. *Nonvolatile memory* or *secondary memory* has usually been handled by *magnetic disks* but personal mobile devices has popularised *flash memory*, a *nonvolatile* semiconductor memory. *Nonvolatile memory* is used to store data and programs between runs and is usually cheaper. *Volatile memory* is faster, more rugged, and more power efficient.

One of the most important *abstractions* is the interface between the hardware and the lowest-level software. The *vocabulary* in which the software communicates to hardware is called the *instruction set architecture* of a computer. The operating system will usually encapsulate the details of doing I/O, allocating memory, and other low-level system functions. The combination of the basic instruction set and the operating system interface provided for application programmers is called the *application binary interface (ABI)*.

Both hardware and software consist of hierarchical layers using *abstraction*, 
with each lower layer hiding details from the level above. One key interface 
between the levels of abstraction is the instruction *set architecture*—the 
interface between the hardware and low-level software. This abstract 
interface enables many *implementations* of varying cost and performance 
to run identical software.

#### Networks

*Networks* interconnect whole computers, allowing computer users to extend the power of computing by including *communication*. Networks have become so popular that they are the backbone of current computer systems. Networked computers have major advantages:
- **Communication**: Information is exchanged between computers at high speeds.
- **Resource sharing**: Rather than each computer having its own I/O devices, computers on the network can share I/O devices.
- **Non-local access**: By connecting computers over long distances, users need not be near the computer they are using.

*Ethernet* is the most popular type of network. It is useful for connecting computers on the same floor of a building, and thus are what is generically called *local area networks* or *LAN*. They are connected with switches that provide routing and security. *Wide area networks* or *WAN* are networks that can connect across continents and are the backbone of the *internet*.
## Performance
There are different ways of defining *performance*. As an individual computer user, you are interested in reducing *response time*—the time between the start and completion of a task. Datacentre managers often care about increasing *throughput* or 
*bandwidth*—the total amount of work done in a given time. Hence, in most cases, we will need different performance metrics as well as different sets of applications to benchmark personal mobile devices, which are more focused on response time, versus servers, which are more focused on throughput.

We can relate performance and execution time for a computer $X$:
$$Performance_{X}=\frac{1}{Execution~time_X}$$
For two computers $X$ and $Y$ we have:
$$Performance_X > Performance_Y$$
$$\frac{1}{Execution~time_X}>\frac{1}{Execution~time_Y}$$
$$Execution~time_Y>Execution~time_X$$
If $X$ is $n$ times as fast as $Y$, then the execution time on $Y$ is $n$ times as long as it is on $X$:
$$\frac{Performance_x}{Performance_Y}=\frac{Execution~time_Y}{Execution~time_X}=n$$
### CPU Performance
Almost all computers are constructed using a clock that determines when events take place in the hardware. These discrete time intervals are called *clock cycles* (or *ticks*, *clocks*, *cycles*). Designers refer to the length of a clock period both as the time for a complete clock cycle (e.g., 250 picoseconds, or 250 ps) and as the clock rate (e.g., 4 gigahertz, or 4 GHz).
$$
\begin{gathered}
\text { CPU execution time }=
\text { CPU clock cycles } \\
\end{gathered} \times \text { Clock cycle time }
$$
Alternatively, because clock rate and clock cycle time are inverses,
$$
\begin{gathered}
\text { CPU execution time } \\
\text { for a program }
\end{gathered}=\frac{\text { CPU clock cycles for a program }}{\text { Clock rate }}
$$

### Instruction performance
*Clock cycles per instruction* or *CPI* is the average number of clock cycles per instruction for a program or program fragment.
$$
\mathrm{CPU} \text { clock cycles }=\text { Instructions for a program } \times \begin{gathered}
\text { Average clock cycles } \\
\text { per instruction }
\end{gathered}
$$
### Classic CPU performance equation
$$
\mathrm{CPU} \text { time }=\text { Instruction count } \times \mathrm{CPI} \times \text { Clock cycle time }
$$
$$
\text { CPU time }=\frac{\text { Instruction count } \times \mathrm{CPI}}{\text { Clock rate }}
$$
### The Big Picture
$$
\text { Time }=\text { Seconds } / \text { Program }=\frac{\text { Instructions }}{\text { Program }} \times \frac{\text { Clock cycles }}{\text { Instruction }} \times \frac{\text { Seconds }}{\text { Clock cycle }}
$$
Always bear in mind that the only complete and reliable measure of computer performance is time. For example, changing the instruction set to lower the instruction count may lead to an organization with a slower clock cycle time or higher CPI that offsets the improvement in instruction count. Similarly, because CPI depends on the type of instructions executed, the code that executes the fewest number of instructions may not be the fastest.
![[Pasted image 20230904105839.png]]
![[Pasted image 20230904105906.png]]
# Lecture notes
![[CompSys/Course Introduction|Course Introduction]]

If we use $k$ bits to represent a number, only $2^k$ distinct values are possible with a binary number system.

Bit shifting

Logic gates (in binary)

C data types

https://education.github.com/

*Representation* and *interpretation


```
#include <stdilib.h>

int main (int argc, char* argv[]){
  ...
  return EXIT_SUCCESS;
}
```
$\texttt{argc}$ is the amount of arguments

$\texttt{EXIT\_SUCCESS}$ ends the program properly

``echo`` returns the last value state


Informally, $\texttt{++a}$ first increments a, then returns its value
Informally, $\texttt{b++}$ first returns the value of b, then increments

A *compiler flag* defines what we don't/do want during compiling (options).

$\texttt{fgets}$ retrieves input from a file

 $\texttt{input}$ to refer to the value
 $\texttt{\&input}$ to refer to the pointer 


``
`sscanf()` scans the value 

Writing to $\texttt{stdout}$:
![[Pasted image 20230904145102.png]]
# Exercises
