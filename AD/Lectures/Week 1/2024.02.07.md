#  Reading notes

## Analyzing Algorithms

*Analyzing* algorithms means predicting the resources that the algorithm requires. Our resource in this case is *computational time*. 

This book assumes that there are no concurrent processes. Every instruction takes the same amount of time (even indexing into arrays).

Each *word* of data has a limit on the number of bits. With inputs of size $n$ we assume that integers are represented by $c~\log_2~n$ bits for some constant $c\leq 1$.

### Analysis of Insertion Sort
![[AD/Lectures/Week 1/2024.02.05#Insertion sort]]

We analyse each line of the pseudocode and examine how long it takes to run.

$n$ is our *input size*.  That is, how many items are being sorted.

The *running time* of the algorithm on a particular input is the number of instructions/data accesses executed.

We assumes that each execution of $k$th line takes $c_k$ time, where $c_k$ is a constant. The *running time* is the sum of running times for each statement executed. A statement is executed $m$ times contributes $c_km$ to the total running time.

The *running time* is denoted by:
$$
T(n)
$$
![[Pasted image 20240131142114.png]]

This gives us the running time for insertion sort:
$$
\begin{aligned}
T(n)= & c_1 n+c_2(n-1)+c_4(n-1)+c_5 \sum_{i=2}^n t_i+c_6 \sum_{i=2}^n\left(t_i-1\right) \\
& +c_7 \sum_{i=2}^n\left(t_i-1\right)+c_8(n-1) .
\end{aligned}
$$

If the array is already sorted the $\texttt{while}$ loop will always evaluate that $key$ is greater than or equal to all values in $\texttt{A[1:i-1]}$ such that the *best-case* running time is given by:
$$
\begin{aligned}
T(n) & =c_1 n+c_2(n-1)+c_4(n-1)+c_5(n-1)+c_8(n-1) \\
& =\left(c_1+c_2+c_4+c_5+c_8\right) n-\left(c_2+c_4+c_5+c_8\right) .
\end{aligned}
$$
The *worst-case* scenario is if the array being sorted is in reverse sorted order. This will cause the algorithm to have to compare each $\texttt{A[i]}$ with each element in the entire sorted subarray $\texttt{A[1:i-1]}$. The worst case is given with:
$$
\begin{aligned}
T(n)= & c_1 n+c_2(n-1)+c_4(n-1)+c_5\left(\frac{n(n+1)}{2}-1\right) \\
& +c_6\left(\frac{n(n-1)}{2}\right)+c_7\left(\frac{n(n-1)}{2}\right)+c_8(n-1) \\
= & \left(\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}\right) n^2+\left(c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8\right) n \\
& -\left(c_2+c_4+c_5+c_8\right) .
\end{aligned}
$$
Note that:
$$
\begin{aligned}
\sum_{i=2}^n i & =\left(\sum_{i=1}^n i\right)-1 \\
& =\frac{n(n+1)}{2}-1
\end{aligned}
$$
and
$$
\begin{aligned}
\sum_{i=2}^n(i-1) & =\sum_{i=1}^{n-1} i \\
& =\frac{n(n-1)}{2}
\end{aligned}
$$
We can represent the *best-case* as a *linear function* of $n$, $an+b$, where $a=c_1+c_2+c_4+c_5+c_8$ and $b=c_2+c_4+c_5+c_8$.

Our *best case* *running time* in this case will be $\mathcal{O}(n)$. This is the case if the array is already sorted, so that we never have to enter the while loop.

We can represent the *worst-case* running time as a *quadratic function* of $n$, $an^2+bn+c$, where $a=\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}$, $b=c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8$, and $c=-(c_2+c_4+c_5+c_8)$.

Our *worst case* *running time* in this case will be $\mathcal{O}(n^2)$. This is the case if the array is in reverse order. In this case, $\texttt{key}$ is always smaller than $\texttt{A[j]}$ and we'll enter the while loop for each iteration of the outer loop.

Finding the *worst-case* running time is usually most indicative of how good an algorithm is. It guarantees that the algorithm will *never* take longer than that running time. In particular cases it can be useful to find the *average-case*.

## Asymptotic running time
To highlight the *order of growth* of a running time we use the notation:
$$
\Theta(n)
$$
In this case, this *worst-case* running time denotes a *linear growth*.

The *worst-case* running time for insertion sort would be written as:
$$
\Theta(n^2)
$$

When we're looking at input sizes large enough to make only the *order of growth* relevant to the *running time* of an algorithm, we say that we are studying its *asymptotic efficiency*. 

We have different notation for different kinds of asymptotic behaviour.

**Theorem 11**. Let $f, g, h, p: \mathbb{R}^{+} \rightarrow \mathbb{R}$ be asymptotically positive functions:
- **(R1)** "Overall constant factors can be ignored" If $c>0$ is a constant then $c f(x)$ is $\Theta(f(x))$.
- **(R2)** "For polynomials only the highest-order term matters" If $p(x)$ is a polynomial of degree $d$, then $p(x)$ is $\Theta\left(x^d\right)$.
- **(R3)** "The fastest growing term determines the growth rate" If $f(x)$ is $o(g(x))$ then $c_1 g(x)+c_2 f(x)$ is $\Theta(g(x))$, where $c_1>0$ and $c_2 \in \mathbb{R}$ are constants.
- **(R4)** "Logarithms grow faster than constants" If $c>0$ is a constant then $c$ is $o\left(\log _a(x)\right)$ for all $a>1$.
- **(R5)** "Powers (and polynomials) grow faster than logarithms" $\log _a(x)$ is $o\left(x^b\right)$ for all $a>1$ and $b>0$.
- **(R6)** "Exponentials grow faster than powers (and polynomials)" $x^a$ is $o\left(b^x\right)$ for all $a$ and all $b>1$.
- **(R7)** "Larger powers grow faster" $x^a$ is $o\left(x^b\right)$ if $a<b$.
- **(R8)** "Exponentials with a bigger base grow faster" $a^x$ is $o\left(b^x\right)$ if $0<a<b$.

### Asymptotic notation
![[Pasted image 20240207155135.png]]
For each notation we assume that $$
g(n): \mathbb{N} \rightarrow \mathbb{R}
$$
is a nonnegative function.
#### $\mathcal{O}$-notation
$\mathcal{O}$-notation characterises an *upper bound* on the asymptotic behaviour of a function. It says that a function grows *no faster* than a certain rate.

For the function $7n^3+100n^2-20n+6$, its highest order term is $7n^3$, such that its rate of growth is $n^3$.

So this function's rate of growth is $\mathcal{O}(n^c)$ for any constant $c\geq3$.

##### Formal definition
$$
\exists c>0~\exists n_0>0:0 \leq f(n) \leq cg(n)~\forall n\geq n_0
$$
#### $\mathcal{o}$-notation
$\mathcal{o}$-notation characterises an *upper bound* that is not *asymptotically tight* .

##### Formal definition
$$
\forall c > 0 ~ \exists n_0 > 0 : 0 \leq f(n) < c g(n) ~ \forall n \geq n_0
$$
#### $\Omega$-notation
$\Omega$-notation characterises a *lower bound* on the asymptotic behaviour of a function. It says that a function grows *as least as fast* as a certain rate.

For the function $7n^3+100n^2-20n+6$, its highest order term is $7n^3$, such that its rate of growth is $n^3$.

So this function's rate of growth is $\Omega(n^c)$ for any constant $c\leq3$.
##### Formal definition
$$\exists c >0~\exists n_{0}>0:~0\leq cg(n)\leq f(n)~\forall n\geq n_{0}$$
#### $\omega$-notation
$\omega$-notation characterises a *lower bound* that is not *asymptotically tight*.

$$
f(n) \in \omega(g(n)) \text { if and only if } g(n) \in \mathcal{o}(f(n)) \text {. }
$$

##### Formal definition
$$\forall c >0~\exists n_{0}>0:~0\leq cg(n)\leq f(n)~\forall n\geq n_{0}$$

#### $\Theta$-notation
$\Theta$-notation characterises a *tight bound* on the asymptotic behaviour of a function. It says that a function grows *precisely* at a certain rate.

If we can show that a function is both $\mathcal{O}(f(n))$ and $\Omega(f(n))$ for some function $f(n)$ that you have shown that the function is $\Theta(f(n))$. 
##### Formal definition
$$\exists c_1 >0~\exists n_{0} >0~\exists c_2 >0: ~0\leq c_{1}g(n)\leq f(n)\leq c_{2}g(n)~\forall n\geq n_{0}$$
#### Theorem 3.1
For any two functions $f(n)$ and $g(n)$, we have:

$f(n)=\Theta(g(n))$ if and only if $f(n)=O(g(n))$ and $f(n)=\Omega(g(n))$.

#### Properties
![[Pasted image 20240207163438.png]]
Because of the properties below we can compare asymptotic notation of comparing two functions $f$ and $g$ and the comparison of two real numbers $a$ and $b$:

$f(n)=O(g(n))$ is like $a \leq b$,
$f(n)=\Omega(g(n))$ is like $a \geq b$,
$f(n)=\Theta(g(n))$ is like $a=b$,
$f(n)=o(g(n))$ is like $a<b$,
$f(n)=\omega(g(n))$ is like $a>b$.

##### Transitivity
$$
\begin{array}{lccc}
f(n) & =\text { and } g(n) & =\operatorname{imply} f(n) & = \\
\Theta(g(n)) & \Theta(h(n)) & \Theta(h(n)), & \\
f(n) & =\text { and } g(n) & =\operatorname{imply} f(n) & = \\
O(g(n)) & O(h(n)) & O(h(n)), & \\
f(n) & =\text { and } g(n) & =\operatorname{imply} f(n) & = \\
\Omega(g(n)) & \Omega(h(n)) & \Omega(h(n)), & \\
f(n) & =\text { and } g(n) & =\operatorname{imply} f(n), & = \\
o(g(n)) & o(h(n)) & o(h(n)), & \\
f(n) & =\text { and } g(n) & =\operatorname{imply} f(n) & = \\
\omega(g(n)) & \omega(h(n)) & \omega(h(n)) . &
\end{array}
$$

##### Reflexivity
$$
\begin{aligned}
& f(n)=\Theta(f(n)), \\
& f(n)=O(f(n)), \\
& f(n)=\Omega(f(n)) .
\end{aligned}
$$

##### Symmetry
$$
f(n)=\Theta(g(n)) \text { if and only if } g(n)=\Theta(f(n)) \text {. }
$$

##### Transpose symmetry
$f \quad(n) \quad=$ if and only $g(n)=\Omega(f(n))$,
$$
\begin{array}{ll}
O(g(n)) & \text { if } \\
f(n)=o(g(n)) & \text { if and only } g(n)=\omega(f \\
& \text { if } \quad(n)) .
\end{array}
$$

### Asymptotic notation in equations/inequalities

For an equation 
$$
4n^2+100n+500=\mathcal{O}(n^2)
$$ the equal sign means set membership, such that: 
$$
4n^2+100n+500 \in \mathcal{O}(n^2)
$$
In the expression:
$$
\sum_{i=1}^{n}\mathcal{O}(i),
$$
there is only a single *anonymous function* of $i$, and thus doesn't equal $\mathcal{O}(1)+\mathcal{O}(2)+\dots+\mathcal{O}(n)$.

For the equation:
$$
2n^2+\Theta(n)=\Theta(n^2)
$$
the asymptotic notation appears on the left-hand side. *No matter how the anonymous function on the left-hand side is chosen, there is a way to choose the anonymous function on the right-hand side to make the equation valid.* 

Thus, for *any* function $f(n)\in\Theta(n)$ there is some function $g(n)\in\Theta(n^2)$ such that $2n^2+f(n)=g(n) ~~\forall n$.

The right-hand side of an equation provides a coarser level of detail than the left-hand side.

Generally, 
- $g(n)=O\left(n^2\right)$ means that $g(n)=f(n)$ for some $f(n) \in O\left(n^2\right)$
- $g(n)=n^2+\Omega(n)$ means $g(n)=n^2+f(n)$ for some $f(n) \in \Omega(n)$


# Lecture notes
![[Pasted image 20240207151538.png]]

## What makes a good algorithm?

- *Correctness*
- *Speed*
- *Space efficiency*
- *Simplicity*
- *Flexibility*

An algorithm is *optimal* if it's not possible to improve it.

## Computational Models

We use *RAM* (*Random Access Machine*) as a simple abstraction of a computer.

Our machine has a constant amount of registers in an array.

The input is stored in the first $n$ $w$-bit words in the memory.

### Running time in RAM-model

Takes one step to execute:
- Arithmetic operation
- Read/write
- Logical operations
- branching/jumps

We usually want to  find the *worst-case* *running time* for an algorithm.

**The analysis can be used for many machines**
![[Pasted image 20240207153424.png]]


In this course we look at single-threaded asymptotic scalability, not optimization nor parallelism.

## Analyzing binary search
![[Pasted image 20240207161754.png]]
## $\mathcal{O}$ arithmetic
### Addition
Suppose that $f(n)=\mathcal{O}(g(n))$ and $f^{\prime}(n)=\mathcal{O}\left(g^{\prime}(n)\right)$ are nonnegative:
- $\exists n_0, c>0:n \geq n_0 \Rightarrow f(n) \leq c g(n)$
- $\exists n_0^{\prime}, c^{\prime}>0 : n \geq n_0^{\prime} \Rightarrow f^{\prime}(n) \leq c^{\prime} g^{\prime}(n)$

$n \geq \max \left(n_0, n_0^{\prime}\right) \Rightarrow f(n)+f^{\prime}(n) \leq c g(n)+c^{\prime} g(n) \leq \max \left(c, c^{\prime}\right) \cdot\left(g(n)+g^{\prime}(n)\right)$

Which means that: $f(n)+f^{\prime}(n)=\mathcal{O}\left(g(n)+g^{\prime}(n)\right)$

### Multiplication
Suppose that $f(n)=\mathcal(O)(g(n))$ og $f^{\prime}(n)=\mathcal{O}\left(g^{\prime}(n)\right)$ are nonnegative, such that:
- $\exists n_0, c>0 : n \geq n_0 \Rightarrow f(n) \leq c g(n)$
- $\exists n_0^{\prime}, c^{\prime}>0 : n \geq n_0^{\prime} \Rightarrow f^{\prime}(n) \leq c^{\prime} g^{\prime}(n)$

$f(n)\cdot f^{\prime}(n)=$$\mathcal{O}(g(n)g^{\prime}(n))$

### Subtraction
Nothing to say ...

### Division
Nothing to say ...
## Simpler analysis of insertion sort

## Example
Suppose $a, b$ are positive whole numbers.

What can we say about $(n+a)^b$ ?

$$
(n+a)^b=n^b+\left(\begin{array}{l}b \\ 1\end{array}\right) a n^{b-1}+\left(\begin{array}{l}b \\ 2\end{array}\right) a^2 n^{b-2}+\ldots+\left(\begin{array}{c}b \\ b-1\end{array}\right) a^{b-1} n+\left(\begin{array}{l}b \\ 0\end{array}\right) a^b
$$
Choose $c=1+\left(\begin{array}{l}b \\ 1\end{array}\right) a+\left(\begin{array}{l}b \\ 2\end{array}\right) a^2+\ldots+\left(\begin{array}{c}b \\ b-1\end{array}\right) a^{b-1}+\left(\begin{array}{l}b \\ 0\end{array}\right) a^b$, and $n^b \leq(n+a)^b \leq c n^b$ for $n \geq 1$

$(n+a)^b=\Theta\left(n^b\right)$

## Functions used in the course
### Floors/ceilings
### Modular arithmetic
### Polynomials
### Exponentials
### Logarithms
#### Iterated logarithm
### Factorials

## $\mathcal{O}$-notation with multivariate functions
$$
f(n, m)=O(g(n, m))
$$
if and only if:
$$
\exists c>0, k>0 : (n \geq k \vee m \geq k) \Rightarrow 0 \leq f(n, m) \leq c g(n, m)
$$

# Exercises
[AD handouts 4/5, April 23 2018]

## 1
*Q*: Prove that $x^2-y^2=1$ has no solutions for positive whole numbers $x,y$.

*A*: We'll try to prove that the statement holds. Use the fact that $x^2-y^2=(x+y)(x-y)$

$$
(x+y)(x-y)=1
$$
We can see now that $y$ is both negative and positive. This contradicts the condition that $y$ is positive.
## 2

*Q*: Prove that $x^2-y^2=10$ has no solutions for positive whole numbers $x,y$.

*A*: We'll try to prove that the statement holds. Use the fact that $x^2-y^2=(x+y)(x-y)$

$$
(x+y)(x-y)=10
$$
We can see now that $y$ is both negative and positive. This contradicts the condition that $y$ is positive.

## 3

*Q*: Show that if $a$ is rational and $b$ is irrational then $a+b$ is an irrational number.

*A*: We can write the equation using fractions and try to show the *contradictory* statement
$$
\frac{x}{y}+b=\frac{z}{w}
$$
We can move the $a$ to the other side
$$
b=\frac{z}{w}-\frac{x}{y}
$$
Subtract the fraction
$$
b=\frac{z-x}{w-y}
$$
We have now shown that $b$ can be represented as a fraction which is a *contradiction* since it is an irrational number.

## 4
*Q*: Give a formula for the sum of the first $n$ odd numbers. (Actually prove via induction?)
$$
f(n)=\sum_{i=1}^n(2 i-1)
$$
*A*: Show via *induction*

**Basis step**
$f(1)=(2\cdot1-1)=1$
Is equal to
$f(1)=1^2=1$

**Induction step**
$f(n+1)=f(n)+2(n+1)-1$
$f(n+1)=f(n)+2n+2-1$
$f(n+1)=f(n)+2n+1$

Is equal to
$f(n+1)=(n+1)^2$
$f(n+1)=n^2+2n+1$

## 5
*Q*: Show that $n!>2^n$ for $n\geq 4$.

*A*:

**Basis step**:
$f_1(4)=4! =24$
$f_2(4)=2^4=16$

Thus: $f_1(4)>f_2(4)$

**Induction step**:
$f_1(n+1)=(n+1)! = n! \cdot (n+1)$
$f_2(n+1)=2^{n+1}=2^n\cdot 2$

Since we know that $n!>2^n$ when $n$ is 4, we can also see that 
$2^{n}\cdot2<n!\cdot (n+1)$ for $n<4$

## 6

Fibonacci numbers are defined as $f_0=0, f_1=1, f_i=f_{i-1}+f_{i-2}, i \geqslant 2$. The first numbers are: $0,1,1,2,3,5,8,13,21, \ldots$.

Show that
$$
\sum_{i=1}^n f_i^2=f_n f_{n+1}
$$

**Basis step** $n=1$:
$f_1^2=1^2=1$
$f_1f_{1+1}=f_1f_{2}=1\cdot1=1$

**Induction step** $n+1$:
$\sum_{i=1}^{n+1} f_i^2=\sum_{i=1}^n f_i^2+f_{n+1}^2$

$f_{n+1} f_{n+1+1}=f_{n+1} f_{n+2}$

Since $f_{n+2}=f_n+f_{n+1}$ 
$f_{n+1} f_{n+2}=f_{n+1} (f_{n}+f_{n+1})=f_{n+1}^2+f_nf_{n+1}$

