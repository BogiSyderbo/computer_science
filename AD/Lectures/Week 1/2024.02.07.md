#  Reading notes

## Analyzing Algorithms

*Analyzing* algorithms means predicting the resources that the algorithm requires. Our resource in this case is *computational time*. 

This book assumes that there are no concurrent processes. Every instruction takes the same amount of time (even indexing into arrays).

Each *word* of data has a limit on the number of bits. With inputs of size $n$ we assume that integers are represented by $c~\log_2~n$ bits for some constant $c\leq 1$.

### Analysis of Insertion Sort
![[2024.02.05#Insertion sort]]

We analyse each line of the pseudocode and examine how long it takes to run.

$n$ is our *input size*.  That is, how many items are being sorted.

The *running time* of the algorithm on a particular input is the number of instructions/data accesses executed.

We assumes that each execution of $k$th line takes $c_k$ time, where $c_k$ is a constant. The *running time* is the sum of running times for each statement executed. A statement is executed $m$ times contributes $c_km$ to the total running time.

The *running time* is denoted by:
$$
T(n)
$$
![[Pasted image 20240131142114.png]]

This gives us the running time for insertion sort:
$$
\begin{aligned}
T(n)= & c_1 n+c_2(n-1)+c_4(n-1)+c_5 \sum_{i=2}^n t_i+c_6 \sum_{i=2}^n\left(t_i-1\right) \\
& +c_7 \sum_{i=2}^n\left(t_i-1\right)+c_8(n-1) .
\end{aligned}
$$

If the array is already sorted the $\texttt{while}$ loop will always evaluate that $key$ is greater than or equal to all values in $\texttt{A[1:i-1]}$ such that the *best-case* running time is given by:
$$
\begin{aligned}
T(n) & =c_1 n+c_2(n-1)+c_4(n-1)+c_5(n-1)+c_8(n-1) \\
& =\left(c_1+c_2+c_4+c_5+c_8\right) n-\left(c_2+c_4+c_5+c_8\right) .
\end{aligned}
$$
The *worst-case* scenario is if the array being sorted is in reverse sorted order. This will cause the algorithm to have to compare each $\texttt{A[i]}$ with each element in the entire sorted subarray $\texttt{A[1:i-1]}$. The worst case is given with:
$$
\begin{aligned}
T(n)= & c_1 n+c_2(n-1)+c_4(n-1)+c_5\left(\frac{n(n+1)}{2}-1\right) \\
& +c_6\left(\frac{n(n-1)}{2}\right)+c_7\left(\frac{n(n-1)}{2}\right)+c_8(n-1) \\
= & \left(\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}\right) n^2+\left(c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8\right) n \\
& -\left(c_2+c_4+c_5+c_8\right) .
\end{aligned}
$$
Note that:
$$
\begin{aligned}
\sum_{i=2}^n i & =\left(\sum_{i=1}^n i\right)-1 \\
& =\frac{n(n+1)}{2}-1
\end{aligned}
$$
and
$$
\begin{aligned}
\sum_{i=2}^n(i-1) & =\sum_{i=1}^{n-1} i \\
& =\frac{n(n-1)}{2}
\end{aligned}
$$
We can represent the *best-case* as a *linear function* of $n$, $an+b$, where $a=c_1+c_2+c_4+c_5+c_8$ and $b=c_2+c_4+c_5+c_8$.

We can represent the *worst-case* running time as a *quadratic function* of $n$, $an^2+bn+c$, where $a=\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}$, $b=c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8$, ad $c=-(c_2+c_4+c_5+c_8)$.

Finding the *worst-case* running time is usually most indicative of how good an algorithm is. It guarantees that the algorithm will *never* take longer than that running time. In particular cases it can be useful to find the *average-case*.

### Asymptotic running time
To highlight the *order of growth* of a running time we use the notation:
$$
\Theta(n)
$$
In this case, this *worst-case* running time denotes a *linear growth*.

The *worst-case* running time for insertion sort would be written as:
$$
\Theta(n^2)
$$

When we're looking at input sizes large enough to make only the *order of growth* relevant to the *running time* of an algorithm, we say that we are studying its *asymptotic efficiency*. 

We have different notation for different kinds of asymptotic behaviour.

#### $\mathcal{O}$-notation
$\mathcal{O}$-notation characterises an *upper bound* on the asymptotic behaviour of a function. It says that a function grows *no faster* than a certain rate.

For the function $7n^3+100n^2-20n+6$, its highest order term is $7n^3$, such that its rate of growth is $n^3$.

So this function's rate of growth is $\mathcal{O}(n^c)$ for any constant $c\geq3$.

#### $\Omega$-notation
	
#### $\Theta$-notation
# Lecture notes
