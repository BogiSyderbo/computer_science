#  Reading notes

## Analyzing Algorithms

*Analyzing* algorithms means predicting the resources that the algorithm requires. Our resource in this case is *computational time*. 

This book assumes that there are no concurrent processes. Every instruction takes the same amount of time (even indexing into arrays).

Each *word* of data has a limit on the number of bits. With inputs of size $n$ we assume that integers are represented by $c~\log_2~n$ bits for some constant $c\leq 1$.

### Analysis of Insertion Sort
![[AD/Lectures/Week 1/2024.02.05#Insertion sort]]

We analyse each line of the pseudocode and examine how long it takes to run.

$n$ is our *input size*.  That is, how many items are being sorted.

The *running time* of the algorithm on a particular input is the number of instructions/data accesses executed.

We assumes that each execution of $k$th line takes $c_k$ time, where $c_k$ is a constant. The *running time* is the sum of running times for each statement executed. A statement is executed $m$ times contributes $c_km$ to the total running time.

The *running time* is denoted by:
$$
T(n)
$$
![[Pasted image 20240131142114.png]]

This gives us the running time for insertion sort:
$$
\begin{aligned}
T(n)= & c_1 n+c_2(n-1)+c_4(n-1)+c_5 \sum_{i=2}^n t_i+c_6 \sum_{i=2}^n\left(t_i-1\right) \\
& +c_7 \sum_{i=2}^n\left(t_i-1\right)+c_8(n-1) .
\end{aligned}
$$

If the array is already sorted the $\texttt{while}$ loop will always evaluate that $key$ is greater than or equal to all values in $\texttt{A[1:i-1]}$ such that the *best-case* running time is given by:
$$
\begin{aligned}
T(n) & =c_1 n+c_2(n-1)+c_4(n-1)+c_5(n-1)+c_8(n-1) \\
& =\left(c_1+c_2+c_4+c_5+c_8\right) n-\left(c_2+c_4+c_5+c_8\right) .
\end{aligned}
$$
The *worst-case* scenario is if the array being sorted is in reverse sorted order. This will cause the algorithm to have to compare each $\texttt{A[i]}$ with each element in the entire sorted subarray $\texttt{A[1:i-1]}$. The worst case is given with:
$$
\begin{aligned}
T(n)= & c_1 n+c_2(n-1)+c_4(n-1)+c_5\left(\frac{n(n+1)}{2}-1\right) \\
& +c_6\left(\frac{n(n-1)}{2}\right)+c_7\left(\frac{n(n-1)}{2}\right)+c_8(n-1) \\
= & \left(\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}\right) n^2+\left(c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8\right) n \\
& -\left(c_2+c_4+c_5+c_8\right) .
\end{aligned}
$$
Note that:
$$
\begin{aligned}
\sum_{i=2}^n i & =\left(\sum_{i=1}^n i\right)-1 \\
& =\frac{n(n+1)}{2}-1
\end{aligned}
$$
and
$$
\begin{aligned}
\sum_{i=2}^n(i-1) & =\sum_{i=1}^{n-1} i \\
& =\frac{n(n-1)}{2}
\end{aligned}
$$
We can represent the *best-case* as a *linear function* of $n$, $an+b$, where $a=c_1+c_2+c_4+c_5+c_8$ and $b=c_2+c_4+c_5+c_8$.

We can represent the *worst-case* running time as a *quadratic function* of $n$, $an^2+bn+c$, where $a=\frac{c_5}{2}+\frac{c_6}{2}+\frac{c_7}{2}$, $b=c_1+c_2+c_4+\frac{c_5}{2}-\frac{c_6}{2}-\frac{c_7}{2}+c_8$, ad $c=-(c_2+c_4+c_5+c_8)$.

Finding the *worst-case* running time is usually most indicative of how good an algorithm is. It guarantees that the algorithm will *never* take longer than that running time. In particular cases it can be useful to find the *average-case*.

### Asymptotic running time
To highlight the *order of growth* of a running time we use the notation:
$$
\Theta(n)
$$
In this case, this *worst-case* running time denotes a *linear growth*.

The *worst-case* running time for insertion sort would be written as:
$$
\Theta(n^2)
$$

When we're looking at input sizes large enough to make only the *order of growth* relevant to the *running time* of an algorithm, we say that we are studying its *asymptotic efficiency*. 

We have different notation for different kinds of asymptotic behaviour.

#### $\mathcal{O}$-notation
$\mathcal{O}$-notation characterises an *upper bound* on the asymptotic behaviour of a function. It says that a function grows *no faster* than a certain rate.

For the function $7n^3+100n^2-20n+6$, its highest order term is $7n^3$, such that its rate of growth is $n^3$.

So this function's rate of growth is $\mathcal{O}(n^c)$ for any constant $c\geq3$.

##### Formal definition
$$
\exists c>0~\exists n_0~0 \leq f(n) \leq cg(n)~\forall n\geq n_0
$$
#### $\Omega$-notation
$\Omega$-notation characterises a *lower bound* on the asymptotic behaviour of a function. It says that a function grows *as least as fast* as a certain rate.

For the function $7n^3+100n^2-20n+6$, its highest order term is $7n^3$, such that its rate of growth is $n^3$.

So this function's rate of growth is $\Omega(n^c)$ for any constant $c\leq3$.
##### Formal definition
$$\exists c,x_{0} \in \mathbb{R}^{+}~0\leq cg(x)\leq f(x)~\forall x\geq x_{0}$$
#### $\Theta$-notation
$\Theta$-notation characterises a *tight bound* on the asymptotic behaviour of a function. It says that a function grows *precisely* at a certain rate.

If we can show that a function is both $\mathcal{O}(f(n))$ and $\Omega(f(n))$ for some function $f(n)$ that you have shown that the function is $\Theta(f(n))$. 
##### Formal definition
$$\exists c_1, x_{0} \in \mathbb{R}^{+} \exists c_2 \in \mathbb{R} ~0\leq c_{1}g(x)\leq f(x)\leq c_{2}g(x)~\forall x\geq x_{0}$$
#### $\mathcal{o}$-notation

### Asymptotic notation in equations/inequalities

For an equation 
$$
4n^2+100n+500=\mathcal{O}(n^2)
$$ the equal sign means set membership, such that: 
$$
4n^2+100n+500 \in \mathcal{O}(n^2)
$$
In the expression:
$$
\sum_{i=1}^{n}\mathcal{O}(i),
$$
there is only a single *anonymous function* of $i$, and thus doesn't equal $\mathcal{O}(1)+\mathcal{O}(2)+\dots+\mathcal{O}(n)$.

For the equation:
$$
2n^2+\Theta(n)=\Theta(n^2)
$$
the asymptotic notation appears on the left-hand side. *No matter how the anonymous function on the left-hand side is chosen, there is a way to choose the anonymous function on the right-hand side to make the equation valid.* 

Thus, for *any* function $f(n)\in\Theta(n)$ there is some function $g(n)\in\Theta(n^2)$ such that $2n^2+f(n)=g(n) ~~\forall n$.

The right-hand side of an equation provides a coarser level of detail than the left-hand side.


# Lecture notes

# Exercises
[AD handouts 4/5, April 23 2018]

## 1
Prove that $x^2-y^2=1$ has no solutions for positive whole numbers $x,y$.

## 2

