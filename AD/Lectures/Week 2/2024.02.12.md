# Reading notes

## Divide-and-conquer

Most *recursive* algorithms follow the *divide-and-conquer*
 method.

This method involves breaking problems into several sub-problems that are similar to the original problem but smaller in size.

The *base case* for a *divide-and-conquer* method is the case where something is solved without recursion.

The *recursive case* has three steps:
- **Divide** (divide into subproblems)
- **Conquer** (solve recursively)
- **Combine** (combine the solutions to form a solution to the problem)

### Analyzing divide and conquer

You can often describe running times of algorithms with *recursive calls* by a *recurrence equation*.

This equation describes the overall running time on a problem of size $n$ in terms of the running time of the same algorithm on smaller inputs.

Let $T(n)$ be the *worst-case* running time of a *divide-and-conquer* algorithm of size $n$.

If for some constant $n_0>0$ such that the problem is small enough $n<n_0$, the straightforward solutions takes constant time $\Theta(1)$.

Suppose that the problem yields $a$ subproblems, each with size $\frac{n}{b}$ or $\frac{1}{b}$ of the original. It takes $T(\frac{n}{b})$ time to solve one subproblem of size $\frac{n}{b}$.

Thus, it takes $aT(\frac{n}{b})$ time to solve all $a$ of them.

If it takes $D(n)$ time to divide the problem into subproblems and $C(n)$ time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence:

$$
T(n)= \begin{cases}\Theta(1) & \text { if } n<n_0, \\ D(n)+a T(n / b)+C(n) & \text { otherwise } .\end{cases}
$$
## Recurrences
A *recurrence* is an equation that describes a function in terms of its value on other, smaller, arguments.

The general form of a *recurrence* is an equation or inequality that describes a function over the integers or reals using the function itself.

It contains two or more *cases*, depending on the argument. If a *case* involves the recursive invocation of the function on different smaller inputs, it is a *recursive case*.

If a case does not involve a recursive invocation, it is a *base case*.

 A *recurrence* $T(n)$ is *algorithmic* if, for every sufficiently large threshold constant $n_0>0$, the following two properties hold:
 
1. For all $n<n_0$, we have $T(n)=\Theta(1)$.
2. For all $n \geq n_0$, every path of recursion terminates in a defined base case within a finite number of recursive invocations.

### Solving recurrences

#### Substitution method

This method comprises two steps:
- Guess the form of the solution using symbolic constants
- Use *mathematical induction* to show that the solution works and find the constants
##### Example
As an example of the substitution method, let's determine an asymptotic upper bound on the recurrence:
$$
T(n)=2 T(\lfloor n / 2\rfloor)+\Theta(n) .
$$
Let's try and prove that this has the *asymptotic* upper bound of $\mathcal{O}(n\lg n)$.

We'll adopt the inductive hypothesis that $T(n) \leq c n \lg n~\forall n \geq$ $n_0$ with constants  $c>0$ and $n_0>0$.

**Base step**:


**Inductive step**:
Assume by induction that this bound holds for all numbers at least as big as $n_0$ and less than $n$. In particular, therefore, if $n \geq 2 n_0$, it holds for $\lfloor n / 2\rfloor$, yielding $T(\lfloor n / 2\rfloor) \leq c\lfloor n / 2\rfloor \lg (\lfloor n / 2\rfloor)$.

Substituting into the recurrence: 
$$
\begin{aligned}
T(n) & \leq 2(c\lfloor n / 2\rfloor \lg (\lfloor n / 2\rfloor))+\Theta(n) \\
& \leq 2(c(n / 2) \lg (n / 2))+\Theta(n) \\
& =c n \lg (n / 2)+\Theta(n) \\
& =c n \lg n-c n \lg 2+\Theta(n) \\
& =c n \lg n-c n+\Theta(n) \\
& \leq c n \lg n,
\end{aligned}
$$
where the last step holds if we constrain the constants $n_0$ and $c$ to be sufficiently large that for $n \geq 2 n_0$, the quantity $c n$ dominates the anonymous function hidden by the $\Theta(n)$ term.

#### Recursion-tree method
Used for generating good guesses

#### Master method

![Abdul Bari](https://www.youtube.com/watch?v=OynWkEj0S-s)

Solve algorithmic recurrences of the form (*master recurrence*)
$$
T(n)=a T(n / b)+f(n),
$$
Where $a>0$ and $b>0$ are constants. We call $f(n)$ the *driving function*.

##### Master theorem
Let $a>0$ and $b>1$ be constants, and let $f(n)$ be a *driving function* that is defined and nonnegative on all sufficiently large reals. Define the recurrence $T(n)$ on $n \in \mathrm{N}$ by
$$
T(n)=a T(n / b)+f(n),
$$
The *asymptotic* behaviour of $T(n)$ can be characterised as follows:
1. If there exists a constant $\epsilon>0$ such that $f(n)=O\left(n^{\log _b a-\epsilon}\right)$, then $T(n)=\Theta\left(n^{\log _b a}\right)$.
2. If there exists a constant $k \geq 0$ such that $f(n)=\Theta\left(n^{\log _b a} \lg ^k n\right)$, then $T(n)=\Theta\left(n^{\log _b a} \lg ^{k+1} n\right)$.
3. If there exists a constant $\epsilon>0$ such that $f(n)=\Omega\left(n^{\log _b a+\epsilon}\right)$, and if $f(n)$ additionally satisfies the regularity condition af $(n / b) \leq c f(n)$ for some constant $c<1$ and all sufficiently large $n$, then $T(n)=$ $\Theta(f(n))$.

# Lecture notes

## Merge sort
$$
\begin{aligned}
& \text { MERGE }(A, p, q, r) \\
& n_1=q-p+1 \\
& n_2=r-q \\
& \text { let } L\left[1 \ldots n_1+1\right] \text { and } R\left[1 \ldots n_2+1\right] \text { be new arrays } \\
& \text { for } i=1 \text { to } n_1 \\
& \quad L[i]=A[p+i-1] \\
& \text { for } j=1 \text { to } n_2 \\
& R[j]=A[q+j] \\
& L\left[n_1+1\right]=\infty \\
& R\left[n_2+1\right]=\infty \\
& i=1 \\
& j=1 \\
& \text { for } k=p \text { to } r \\
& \quad \text { if } L[i] \leq R[j] \\
& \quad A[k]=L[i] \\
& \quad i=i+1 \\
& \quad \text { else } A[k]=R[j] \\
& \quad j=j+1
\end{aligned}
$$

 *Merge* is an algorithm that merges two sorted arrays
$p$ is the first elemnt
$q$ is some middle element
$r$ is the last element

Time complexity
$\mathcal{O}(1+r-p)$

*Merge-sort* is an algorithm for recursively sorting sublists and merging them
$$
\begin{aligned}
& \operatorname{MERGE-SORT}(A, p, r) \\
& \text { if } p<r \\
& q=\lfloor(p+r) / 2\rfloor \\
& \text { Merge-Sort }(A, p, q) \\
& \text { Merge-Sort }(A, q+1, r) \\
& \operatorname{Merge}(A, p, q, r) \\
&
\end{aligned}
$$

$Tn+Tfloorn/2+Tceilingn/2+On$

## Quicksort
## Maksimum deltabel 

## Time complexity of divide-and-conquer
### Recursion tree
![[Pasted image 20240212161803.png]]
![[Pasted image 20240212161810.png]]

1. $T(n) \leq 4 T(n / 2)+n$

Rekursionstræet har dybde $\log _2 n$, sum $4^i$ på niveau $i$, totalt $4^{\log _2 n}=O\left(n^2\right)$ Det totale arbejde stiger med rekursionsniveauet

Faktiskt $2^i$?

2. $T(n) \leq 4 T(n / 4)+n$ :

Rekursionstræet har dybde $\log _4 n$, sum $n$ på hvert niveau, totalt $O(n \log n)$ Det totale arbejde er det samme på alle rekursionsniveauerne

3. $T(n) \leq 2 T(n / 2)+n^2$

Rekursionstræet har dybde $\log _2 n$, sum $n^2 / 2^i$ på niveau $i$, totalt $O\left(n^2\right)$ Det totale arbejde falder med rekursionsniveauet


### Master method
Provides a solution to the most common recursion equations, in the form:
$T(n)=a T(n / b)+\Theta(f(n))$, for constants $a \geq 1$ and $b>1$

Let $\varepsilon>0$ be a constant and suppose that $f(n)$ is a growing function. Then it holds that:
1. $f(n)=O\left(n^{\log _b(a)-\varepsilon}\right) \Longrightarrow T(n)=\Theta\left(n^{\log _b a}\right)$
2. $f(n)=\Theta\left(n^{\log _b(a)}\right) \Longrightarrow T(n)=\Theta\left(n^{\log _b a} \log n\right)$
3. $f(n)=\Omega\left(n^{\log _b(a)+\varepsilon}\right) \Longrightarrow T(n)=\Theta(f(n))$
![[Pasted image 20240213231618.png]]
$$
\text { Total: } \Theta\left(n^{\log _b a}\right)+\sum_{j=0}^{\log _b n-1} a^j f\left(n / b^j\right)
$$

# Exercises
ADS exercises 1 and 2

## 3-1 Asymptotic behaviour of polynomials
Let
$$
p(n)=\sum_{i=0}^d a_i n^i,
$$
where $a_d>0$, be a degree- $d$ polynomial in $n$, and let $k$ be a constant. Use the definitions of the asymptotic notations to prove the following properties.
- a. If $k \geq d$, then $p(n)=O\left(n^k\right)$.

If we look at the exponent in the sum,  $\sum_{i=0}^d n^i$, if we have a case where $\sum_{i=0}^k n^i$ where $k\geq d$ then it follows that $n^d$ is upper bounded by $n^k$.

- b. If $k \leq d$, then $p(n)=\Omega\left(n^k\right)$.

Same as *a* but this time it's lower bounded.

- c. If $k=d$, then $p(n)=\Theta\left(n^k\right)$.
- d. If $k>d$, then $p(n)=o\left(n^k\right)$.
- e. If $k<d$, then $p(n)=\omega\left(n^k\right)$.


## 3-2
Indicate, for each pair of expressions $(A, B)$ in the table below whether $A$ is $O, o, \Omega, \omega$, or $\Theta$ of $B$. Assume that $k \geq 1, \epsilon>0$, and $c>1$ are constants. Write your answer in the form of the table with "yes" or "no" written in each box.

$$
\begin{array}{|c|c|c|c|c|c|c|}
\hline \text { A } & B & O & o & \Omega & \omega & \Theta \\
\hline \lg ^k n & n^\epsilon & Y & Y & Y & Y & Y \\
\hline n^k & c^n & Y & Y & Y & Y & Y \\
\hline \sqrt{n} & n^{\sin n} & Y & Y & Y & Y & Y \\
\hline 2^n & 2^{n / 2} & Y & Y & Y & Y & Y \\
\hline n^{\lg c} & c^{\lg n} & Y & Y & Y & Y & Y \\
\hline \lg (n !) & \lg \left(n^n\right) & Y & Y & Y & Y & Y \\
\hline
\end{array}
$$
