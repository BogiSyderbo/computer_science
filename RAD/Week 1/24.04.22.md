# Reading and lecture

## Conditional Probability
![[Pasted image 20240423141713.png]]

Two events $\mathcal{E}_1$ and $\mathcal{E}_2$ are said to be *independent* if the probability that they both occur is given by:
$$
\operatorname{Pr}\left[\mathcal{E}_1 \cap \mathcal{E}_2\right]=\operatorname{Pr}\left[\mathcal{E}_1\right] \times \operatorname{Pr}\left[\mathcal{E}_2\right]
$$
In the more general case where $\mathcal{E}_1$ and $\mathcal{E}_2$ are not necessarily *independent*,
$$
\operatorname{Pr}\left[\mathcal{E}_1 \cap \mathcal{E}_2\right]=\operatorname{Pr}\left[\mathcal{E}_1 \mid \mathcal{E}_2\right] \times \operatorname{Pr}\left[\mathcal{E}_2\right]=\operatorname{Pr}\left[\mathcal{E}_2 \mid \mathcal{E}_1\right] \times \operatorname{Pr}\left[\mathcal{E}_1\right],
$$
where $\operatorname{Pr}\left[\mathcal{E}_1 \mid \mathcal{E}_2\right]$ denotes the *conditional probability* of $\mathcal{E}_1$ given $\mathcal{E}_2$. 

Sometimes, when a collection of events is not independent, a convenient method for computing the probability of their intersection is to use the following generalization:
$$
\operatorname{Pr}\left[\cap_{i=1}^k \mathcal{E}_i\right]=\operatorname{Pr}\left[\mathcal{E}_1\right] \times \operatorname{Pr}\left[\mathcal{E}_2 \mid \mathcal{E}_1\right] \times \operatorname{Pr}\left[\mathcal{E}_3 \mid \mathcal{E}_1 \cap \mathcal{E}_2\right] \cdots \operatorname{Pr}\left[\mathcal{E}_k \mid \cap_{i=1}^{k-1} \mathcal{E}_i\right]
$$

## Union bound
For $k$ events $\mathcal{E}_1, \ldots, \mathcal{E}_k$,
$$
\operatorname{Pr}\left[\cup_{i=1}^k \mathcal{E}_i\right] \leq \sum_{i=1}^k \operatorname{Pr}\left[\mathcal{E}_i\right]
$$

This is called a *union bound*.

If the events are disjoint (i.e. $\mathcal{E}_i \cap \mathcal{E}_j=\emptyset$ for $i \neq j$ ),
$$
\operatorname{Pr}\left[\cup_{i=1}^k \mathcal{E}_i\right]=\sum_{i=1}^k \operatorname{Pr}\left[\mathcal{E}_i\right]
$$

## Probability of intersection of events


For any events $\mathcal{E}_1$ and $\mathcal{E}_2$,
$$
\operatorname{Pr}\left[\mathcal{E}_1 \cap \mathcal{E}_2\right]=\operatorname{Pr}\left[\mathcal{E}_1 \mid \mathcal{E}_2\right] \cdot \operatorname{Pr}\left[\mathcal{E}_2\right]=\operatorname{Pr}\left[\mathcal{E}_2 \mid \mathcal{E}_1\right] \cdot \operatorname{Pr}\left[\mathcal{E}_1\right] .
$$

Generally for $k$ events $\mathcal{E}_1, \ldots, \mathcal{E}_k$,
$$
\operatorname{Pr}\left[\cap_{i=1}^k \mathcal{E}_i\right]=\operatorname{Pr}\left[\mathcal{E}_1\right] \cdot \operatorname{Pr}\left[\mathcal{E}_2 \mid \mathcal{E}_1\right] \cdot \operatorname{Pr}\left[\mathcal{E}_3 \mid \mathcal{E}_1 \cap \mathcal{E}_2\right] \ldots \operatorname{Pr}\left[\mathcal{E}_k \mid \cap_{i=1}^{k-1} \mathcal{E}_i\right]
$$

Even more generally:
$$
\operatorname{Pr}\left[\cap_{i=1}^k \mathcal{E}_i\right]=\prod_{i=1}^k \operatorname{Pr}\left[\mathcal{E}_i \mid \cap_{j=1}^{i-1} \mathcal{E}_j\right]
$$

## Random Variables
A *random variable* $X$ is a function of  sample space $\Omega$ such that: $X: \Omega \rightarrow \mathbb{R}$

It is a function that maps the sample space to the real numbers.

### Indicator variable
An indicator variable is a random variable mapping to $\{0,1\}$.

For an indicator variable $Z$, we can talk about the indicated event $\{\omega \in \Omega \mid Z(\omega)=1\}$, which is usually just written as $Z=1$.

### Expectation of a random variable
The *expectation* of a random variable $X$ is defined as
$$
\mathbb{E}[X]=\sum_x x \operatorname{Pr}[X=x],
$$
#### Linearity of expectation
Given two random variables $X$ and $Y$, we have
$$
\mathbb{E}[X+Y]=\mathbb{E}[X]+\mathbb{E}[Y]
$$

This generalizes to any number of variables $X_1, \ldots, X_k$ :
$$
\mathbb{E}\left[\sum_{i=1}^k X_i\right]=\sum_{i=1}^k \mathbb{E}\left[X_i\right]
$$

## Min-cut Algorithm

Let $G=(V,E)$ be a connected, undirected multigraph with $n$ vertices.

Find the smallest $C\subseteq E$ that splits $G$. In other words, cut the graph through the fewest edges

$C$ is called the *min-cut* and $\lambda(G)=|C|$ is the *min-cut* size, also known as the *edge connectivity* of $G$.

![[Pasted image 20240422143953.png]]

Algorithm:
![[Pasted image 20240423142339.png]]
![[Pasted image 20240423142407.png]]
[More about min-cut](24.04.24#Min-cut continuation)
## Harmonic numbers
$$
H_n=1+\frac{1}{2}+\frac{1}{3}+\cdots+\frac{1}{}=\sum_{k=1}^n \frac{1}{k}n
$$
is called the  $n$th Harmonic number.



Proposition B.4:
$$
H_n=\ln n+\Theta(1) \in \mathcal{O}(\log n) .
$$

# Exercises

Suppose that you're playing with a uniform and fair dice.

**1: Probability of there not being any sixes after two independent rolls?**

Chance of not rolling 6 is $\frac{5}{6}$

Law of Total Probability:

$\frac{5}{6}\cdot\frac{5}{6}=\frac{25}{36}$

**2:** **Probability of there being at least one six after two independent rolls?**

The total probability 1 minus the probability of there not being any sixes:
$\frac{36}{36}-\frac{25}{36}=\frac{11}{36}$

**3: Probability of hitting 2 sixes after two independent rolls**

The probability of hitting a six is $\frac{1}{6}$

Law of Total Probability:

$\frac{1}{6}\cdot \frac{1}{6}=\frac{1}{36}$ 

**4: Probability of hitting exactly one six after two independent rolls**

There are 12 combinations where 

$\frac{36}{36}-(\frac{36}{36}-\frac{12}{36})=\frac{12}{36}$

**5: Probability of hitting two of the same after two indpendent rolls**

Each roll has a probability of $\frac{1}{6}$ of hitting a value. Since it doesn't matter what the value is and we have six possible values, we multiply the probability by six:
$$\frac{1}{6}\cdot\frac{1}{6}\cdot6=\frac{6}{36}=\frac{1}{6}$$

**6: Probability of the sum of the values of two rolls being 7**

1+6=7
2+5=7
3+4=7

Three combinations of dice sum up to 7. Since order doesn't matter, we say six combinations sum up to 7. We have 36 po ssible combinations.

$\frac{6}{36}=\frac{1}{6}$


**7: Probability of the sum of two rolls being larger or equal to 10**
Possible combinations:
6+6=12
6+5=11
6+4=10
5+5=10

8 possible combinations (order doesn't matter)

$\frac{8}{36}$
