# Reading notes

## Random variables
![[Pasted image 20240208225633.png]]
A *random variable* maps the *sample space* into the real line. 

**Definition 3.1.1** (*Random variable*). Given an experiment with sample space $S$, a random variable (r.v.) is a function from the sample space $S$ to the real numbers $\mathbb{R}$. It is common, but not required, to denote random variables by capital letters.

**Definition 3.2.1** (*Discrete random variable*). A random variable $X$ is said to be discrete if there is a finite list of values $a_1, a_2, \ldots, a_n$ or an infinite list of values $a_1, a_2, \ldots$ such that $P\left(X=a_j\right.$ for some $\left.j\right)=1$. If $X$ is a discrete r.v., then the finite or countably finite set of values $x$ such that $P(X=x)>0$ is called the *support* of $X$.

## Probability mass function

**Definition 3.2.2** (*Probability mass function*). The probability mass function (PMF) of a discrete r.v. $X$ is the function $p_X$ given by $p_X(x)=P(X=x)$. 

In writing $P(X=x)$, we are using $X=x$ to denote an *event*, consisting of all outcomes $s$ to which $X$ assigns the number $x$.

This event is also written as $\{X=x\}$; formally, $\{X=x\}$ is defined as $\{s \in S: X(s)=x\}$.

If $X$ is the number of Heads in two fair coin tosses, then $\{X=1\}$ consists of the sample outcomes $H T$ and $T H$, which are the two outcomes to which $X$ assigns the number 1 . Since $\{H T, T H\}$ is a subset of the *sample space*, it is an *event*. So it makes sense to talk about $P(X=1)$, or more generally, $P(X=x)$. If $\{X=x\}$ were anything other than an event, it would make no sense to calculate its probability! It does not make sense to write " $P(X)$ "; we can only take the probability of an event, not of an r.v.
![[Pasted image 20240208233301.png]]
![[Pasted image 20240208230428.png]]**Theorem 3.2.7** (*Valid PMFs*). Let $X$ be a discrete r.v. with support $x_1, x_2, \ldots$ 
The PMF $p_X$ of $X$ must satisfy the following two criteria:
- Nonnegative: $p_X(x)>0$ if $x=x_j$ for some $j$, and $p_X(x)=0$ otherwise;
- Sums to $1: \sum_{j=1}^{\infty} p_X\left(x_j\right)=1$.

## Independence of events
**Definition 2.5.1** (*Independence of two events*). Events $A$ and $B$ are independent if
$$
P(A \cap B)=P(A) P(B) .
$$

If $P(A)>0$ and $P(B)>0$, then this is equivalent to
$$
P(A \mid B)=P(A)
$$
$A$ and $B$ are *independent* if learning that $B$ occurred gives us no information that would change our probabilities for $A$ occurring (and vice versa).

 *Independence* is completely different from *disjointness*. If $A$ and $B$ are disjoint, then $P(Aâˆ©B) = 0$, so disjoint events can be independent only if $P(A) = 0$ or $P(B) = 0$. 
 
 Knowing that $A$ occurs tells us that $B$ definitely did not occur, so $A$ clearly conveys information about $B$, meaning the two events are not independent.

**Proposition 2.5.3**. If $A$ and $B$ are independent, then $A$ and $B^c$ are independent, $A^c$ and $B$ are independent, and $A^c$ and $B^c$ are independent.

**Definition 2.5.6** (*Independence of many events*). For $n$ events $A_1, A_2, \ldots, A_n$ to be independent, we require any pair to satisfy $P\left(A_i \cap A_j\right)=P\left(A_i\right) P\left(A_j\right)$ (for $i \neq j$ ), any triplet to satisfy $P\left(A_i \cap A_j \cap A_k\right)=P\left(A_i\right) P\left(A_j\right) P\left(A_k\right)$ (for $i, j, k$ distinct), and similarly for all quadruplets, quintuplets, and so on. This can quickly become unwieldy, but later we will discuss other ways to think about independence. For infinitely many events, we say that they are independent if every finite subset of the events is independent. Conditional independence is defined analogously to independence.
# Lecture notes



*Joint independence* does not guarantee *pairwise independence*:
$$
P(A\cap B\cap C)=P(A)P(B)
P(C)$$
**Definition 2.5.4** (*Independence of three events*). Events $A, B$, and $C$ are said to be independent if all of the following equations hold:
$$
\begin{aligned}
P(A \cap B) & =P(A) P(B), \\
P(A \cap C) & =P(A) P(C), \\
P(B \cap C) & =P(B) P(C), \\
P(A \cap B \cap C) & =P(A) P(B) P(C) .
\end{aligned}
$$




A real *stochastic/random variable* $X$ is a function $X:S\rightarrow \mathbb{R}$ for sample space $S$ with probability $P$. The "randomness" comes from the sample space and not from $X$ itself.

**Theorem**: Let $S$ be a sample space with probability $P$ and $X:S\rightarrow \mathbb{R}$ is a *random variable*. $P_x$ is defined as:
$$
P_x(A)=(X\in A)
$$
for $A\subseteq \mathbb{R}$. $P_x$ is a *probability distribution* on $\mathbb{R}$.

$X$ pushes the *probability distribution* from $P$ to $S$ onto $\mathbb{R}$

The *support* of a *discrete random variable* is the smallest amount $C\subseteq \mathbb{R}$ such that $P(X\in C)=1$, where $C=\{x\in \mathbb{R}\mid P(X=x)>0\}$

**PMF**: Let $X$ be a d.r.v. *PMF* for $X$ ...


## Important discrete distributions

### Discrete uniform

### Bernoulli

### Indicator variable
### Binomial

### Hypergeometric

### Poisson

### Geometric
