# Reading notes
[BOH] 2, 3.1, 3.2

## Conditional probability
Probability of *event* $R$ given *event* $C$
$P(R\mid C)$

*Probability of rain given clouds*

We say that we are *conditioning* on $C$.

We can continually update our probability
$$
P\left(R \mid B_1, \ldots, B_n\right)
$$
If it starts raining, our *conditional probability* becomes 1.

**Definition 2.2.1** (*Conditional probability*). If $A$ and $B$ are events with $P(B)>0$, then the conditional probability of $A$ given $B$, denoted by $P(A \mid B)$, is defined as
$$
P(A \mid B)=\frac{P(A \cap B)}{P(B)} .
$$

We call $P(A)$ the *prior probability* of $A$ and $P(A\mid B)$ the *posterior probability* of $A$.

For an event $E$:
- Conditional probabilities are between 0 and 1 .
- $P(S \mid E)=1, P(\emptyset \mid E)=0$.
- If $A_1, A_2, \ldots$ are disjoint, then $P\left(\cup_{j=1}^{\infty} A_j \mid E\right)=\sum_{j=1}^{\infty} P\left(A_j \mid E\right)$.
- $P\left(A^c \mid E\right)=1-P(A \mid E)$.
- Inclusion-exclusion: $P(A \cup B \mid E)=P(A \mid E)+P(B \mid E)-P(A \cap B \mid E)$.

## Intersection

**Theorem 2.3.1** (*Probability of the intersection of two events*). For any events $A$ and $B$ with positive probabilities,
$$
P(A \cap B)=P(B) P(A \mid B)=P(A) P(B \mid A)
$$
**Theorem 2.3.2** (*Probability of the intersection of $n$ events*). For any events $A_1, \ldots, A_n$ with $P\left(A_1, A_2, \ldots, A_{n-1}\right)>0$,
$$
P\left(A_1, A_2, \ldots, A_n\right)=P\left(A_1\right) P\left(A_2 \mid A_1\right) P\left(A_3 \mid A_1, A_2\right) \cdots P\left(A_n \mid A_1, \ldots, A_{n-1}\right)
$$

## Bayes' rule

**Theorem 2.3.3** (*Bayes' rule*).
$$
P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}
$$
 ![](https://miro.medium.com/v2/resize:fit:679/0*tvBMtD2-mkjznBBV.gif)
**Definition 2.3.4** (*Odds*). The odds of an event $A$ are
$$
\operatorname{odds}(A)=P(A) / P\left(A^c\right) .
$$
**Theorem 2.3.5** (*Odds form of Bayes' rule*). For any events $A$ and $B$ with positive probabilities, the odds of $A$ after conditioning on $B$ are
$$
\frac{P(A \mid B)}{P\left(A^c \mid B\right)}=\frac{P(B \mid A)}{P\left(B \mid A^c\right)} \frac{P(A)}{P\left(A^c\right)}
$$

*The law of total probability* (LOTP) relates *conditional probability* to *unconditional probability*. It is essential for fulfilling the promise that *conditional probability* can be used to decompose complicated probability problems into simpler pieces, and it is often used in tandem with *Bayesâ€™ rule*.

## LOTP
**Theorem 2.3.6** (*Law of total probability*). Let $A_1, \ldots, A_n$ be a partition of the sample space $S$ (i.e., the $A_i$ are disjoint events and their union is $S$ ), with $P\left(A_i\right)>0$ for all $i$. Then
$$
P(B)=\sum_{i=1}^n P\left(B \mid A_i\right) P\left(A_i\right)
$$
## Extra conditioning
**Theorem 2.4.2** (*Bayes' rule with extra conditioning*). Provided that $P(A \cap E)>0$ and $P(B \cap E)>0$, we have
$$
P(A \mid B, E)=\frac{P(B \mid A, E) P(A \mid E)}{P(B \mid E)} .
$$

**Theorem 2.4.3** (*LOTP with extra conditioning*). Let $A_1, \ldots, A_n$ be a partition of $S$. Provided that $P\left(A_i \cap E\right)>0$ for all $i$, we have
$$
P(B \mid E)=\sum_{i=1}^n P\left(B \mid A_i, E\right) P\left(A_i \mid E\right) .
$$

# Slides notes

# Exercises
## SS.1

We have three coins (green, red, blue). The green is symmetrical, while the probability of tails is 0.3 for the red coin and 0.4 for the blue coin.

We perform the following experiment: We first toss the green coin. If we get heads, we throw then the red coin; otherwise we toss the blue coin. We register at the end whether we got a tails or heads in the second toss, which was therefore either with the red or the blue coin.

### 1
Explain the following conditional probabilities

$$
\begin{gathered}
P(\text { tails } \mid \text { red })=0.3 \\
P(\text { tails } \mid \text { blue })=0.4 \\
P(\text { red })=P(\text { blue })=0.5
\end{gathered}
$$

The *probability* of*event* tails *conditioned* on red is $0.3$

The *probability* of *event* tails conditioned on blue is 0.4

The *probability* of *event* red or blue is 0.5, since it is decided after we've tossed the green coin which is *symmetrical*
### 2
Decide $P(\text{tails})$ by using LOTP
![[2024.02.09#LOTP]]

$$
\begin{aligned}
P(\text{tails}) & = P(T\mid R)P(R)+P(T\mid B)P(B) \\
 & = 0.3 \cdot 0.5+0.4\cdot0.5 \\
 & = 0.35
\end{aligned}
$$ 
### 3
Decide $P(R\mid \text{tails})$ by using Bayes' rule
![[2024.02.09#Bayes' rule]]

$$
\begin{aligned}
P(R \mid T)&=\frac{P(T \mid R) P(R)}{P(T)} \\
& = \frac{0.3\cdot0.5}{0.35} \\
& = 0.43
\end{aligned}
$$
## SS.4
Look at R files

## BH 1.56
### a
Binomial coefficient:
$\left[\begin{matrix}10 \\ 5\end{matrix}\right]=252$

$\frac{10\cdot9\cdot8\cdot7\cdot6}{5!}=252$

Binomial coefficient:
$\left[\begin{matrix}10 \\ 6\end{matrix}\right]=210$
### b
$\left[\begin{matrix}10 \\ 5\end{matrix}\right]\cdot\frac{1}{2}=126$


$\left[\begin{matrix}10 \\ 6\end{matrix}\right]$ or $\left[\begin{matrix}10 \\ 4\end{matrix}\right]$=$210$ ?
### c

$S=\{(x_1,x_2,x_3)\mid x_1,x_2,x_3 \in \{1,\dots,365\}\}$

*Cardinality* of $S$:
$|S|=365^3$ via multiplication principle

1. 

Only one triple $(1,1,1)$, thus:
$$
\frac{1}{365^3}
$$

2. 

The triple $(1,2,3)$ and *permutations* of it

$3! = 6$

$\frac{6}{365^3}$

## BH 1.44
$A\subseteq B \Rightarrow P(B-A)=P(B)-P(A)$

$B=B-A\mathop{\dot{\cup}} A$

Using axiom 1.6.1(2)
![[SS/SSB3/Lectures/Week 1/2024.02.05#General definition of probability]]

$$
\begin{aligned}
P(B) &=P(B-A\mathop{\dot{\cup}}A) \\
&= P(B-A)+P(A) \\
&= P(B-A) \\
&= P(B)-P(A)
\end{aligned}
$$

## BH 1.38
### a
$$
\frac{12\cdot2\cdot10!}{12!}=\frac{2}{11}
$$
### b
The sample space is the binomial coefficient $\left[\begin{matrix}12 \\ 2\end{matrix}\right]$
$$
\frac{12}{\left[\begin{matrix}12 \\ 2\end{matrix}\right]}=\frac{12}{\frac{12\cdot11}{2}}=\frac{2\cdot12}{12\cdot11}=\frac{2}{11}
$$