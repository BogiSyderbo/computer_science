# Reading notes
[BOH] 2, 3.1, 3.2

## Conditional probability
Probability of *event* $R$ given *event* $C$
$P(R\mid C)$

*Probability of rain given clouds*

We say that we are *conditioning* on $C$.

We can continually update our probability
$$
P\left(R \mid B_1, \ldots, B_n\right)
$$
If it starts raining, our *conditional probability* becomes 1.

**Definition 2.2.1** (*Conditional probability*). If $A$ and $B$ are events with $P(B)>0$, then the conditional probability of $A$ given $B$, denoted by $P(A \mid B)$, is defined as
$$
P(A \mid B)=\frac{P(A \cap B)}{P(B)} .
$$

We call $P(A)$ the *prior probability* of $A$ and $P(A\mid B)$ the *posterior probability* of $A$.

For an event $E$:
- Conditional probabilities are between 0 and 1 .
- $P(S \mid E)=1, P(\emptyset \mid E)=0$.
- If $A_1, A_2, \ldots$ are disjoint, then $P\left(\cup_{j=1}^{\infty} A_j \mid E\right)=\sum_{j=1}^{\infty} P\left(A_j \mid E\right)$.
- $P\left(A^c \mid E\right)=1-P(A \mid E)$.
- Inclusion-exclusion: $P(A \cup B \mid E)=P(A \mid E)+P(B \mid E)-P(A \cap B \mid E)$.

## Intersection

**Theorem 2.3.1** (*Probability of the intersection of two events*). For any events $A$ and $B$ with positive probabilities,
$$
P(A \cap B)=P(B) P(A \mid B)=P(A) P(B \mid A)
$$
**Theorem 2.3.2** (*Probability of the intersection of $n$ events*). For any events $A_1, \ldots, A_n$ with $P\left(A_1, A_2, \ldots, A_{n-1}\right)>0$,
$$
P\left(A_1, A_2, \ldots, A_n\right)=P\left(A_1\right) P\left(A_2 \mid A_1\right) P\left(A_3 \mid A_1, A_2\right) \cdots P\left(A_n \mid A_1, \ldots, A_{n-1}\right)
$$

## Bayes' rule

**Theorem 2.3.3** (*Bayes' rule*).
$$
P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}
$$
 ![](https://miro.medium.com/v2/resize:fit:679/0*tvBMtD2-mkjznBBV.gif)
**Definition 2.3.4** (*Odds*). The odds of an event $A$ are
$$
\operatorname{odds}(A)=P(A) / P\left(A^c\right) .
$$
**Theorem 2.3.5** (*Odds form of Bayes' rule*). For any events $A$ and $B$ with positive probabilities, the odds of $A$ after conditioning on $B$ are
$$
\frac{P(A \mid B)}{P\left(A^c \mid B\right)}=\frac{P(B \mid A)}{P\left(B \mid A^c\right)} \frac{P(A)}{P\left(A^c\right)}
$$

*The law of total probability* (LOTP) relates *conditional probability* to *unconditional probability*. It is essential for fulfilling the promise that *conditional probability* can be used to decompose complicated probability problems into simpler pieces, and it is often used in tandem with *Bayes’ rule*.

## LOTP
**Theorem 2.3.6** (*Law of total probability*). Let $A_1, \ldots, A_n$ be a partition of the sample space $S$ (i.e., the $A_i$ are disjoint events and their union is $S$ ), with $P\left(A_i\right)>0$ for all $i$. Then
$$
P(B)=\sum_{i=1}^n P\left(B \mid A_i\right) P\left(A_i\right)
$$
## Extra conditioning
**Theorem 2.4.2** (*Bayes' rule with extra conditioning*). Provided that $P(A \cap E)>0$ and $P(B \cap E)>0$, we have
$$
P(A \mid B, E)=\frac{P(B \mid A, E) P(A \mid E)}{P(B \mid E)} .
$$

**Theorem 2.4.3** (*LOTP with extra conditioning*). Let $A_1, \ldots, A_n$ be a partition of $S$. Provided that $P\left(A_i \cap E\right)>0$ for all $i$, we have
$$
P(B \mid E)=\sum_{i=1}^n P\left(B \mid A_i, E\right) P\left(A_i \mid E\right) .
$$
## Independence of events
**Definition 2.5.1** (*Independence of two events*). Events $A$ and $B$ are independent if
$$
P(A \cap B)=P(A) P(B) .
$$

If $P(A)>0$ and $P(B)>0$, then this is equivalent to
$$
P(A \mid B)=P(A)
$$
$A$ and $B$ are *independent* if learning that $B$ occurred gives us no information that would change our probabilities for $A$ occurring (and vice versa).

 *Independence* is completely different from *disjointness*. If $A$ and $B$ are disjoint, then $P(A∩B) = 0$, so disjoint events can be independent only if $P(A) = 0$ or $P(B) = 0$. 
 
 Knowing that $A$ occurs tells us that $B$ definitely did not occur, so $A$ clearly conveys information about $B$, meaning the two events are not independent.

**Proposition 2.5.3**. If $A$ and $B$ are independent, then $A$ and $B^c$ are independent, $A^c$ and $B$ are independent, and $A^c$ and $B^c$ are independent.

## Random variables
![[Pasted image 20240208225633.png]]
A *random variable* maps the *sample space* into the real line. 

**Definition 3.1.1** (*Random variable*). Given an experiment with sample space $S$, a random variable (r.v.) is a function from the sample space $S$ to the real numbers $\mathbb{R}$. It is common, but not required, to denote random variables by capital letters.

**Definition 3.2.1** (*Discrete random variable*). A random variable $X$ is said to be discrete if there is a finite list of values $a_1, a_2, \ldots, a_n$ or an infinite list of values $a_1, a_2, \ldots$ such that $P\left(X=a_j\right.$ for some $\left.j\right)=1$. If $X$ is a discrete r.v., then the finite or countably finite set of values $x$ such that $P(X=x)>0$ is called the *support* of $X$.

## Probability mass function

**Definition 3.2.2** (*Probability mass function*). The probability mass function (PMF) of a discrete r.v. $X$ is the function $p_X$ given by $p_X(x)=P(X=x)$. 

In writing $P(X=x)$, we are using $X=x$ to denote an *event*, consisting of all outcomes $s$ to which $X$ assigns the number $x$.

This event is also written as $\{X=x\}$; formally, $\{X=x\}$ is defined as $\{s \in S: X(s)=x\}$, but writing $\{X=x\}$ is shorter and more intuitive.

If $X$ is the number of Heads in two fair coin tosses, then $\{X=1\}$ consists of the sample outcomes $H T$ and $T H$, which are the two outcomes to which $X$ assigns the number 1 . Since $\{H T, T H\}$ is a subset of the *sample space*, it is an *event*. So it makes sense to talk about $P(X=1)$, or more generally, $P(X=x)$. If $\{X=x\}$ were anything other than an event, it would make no sense to calculate its probability! It does not make sense to write " $P(X)$ "; we can only take the probability of an event, not of an r.v.
![[Pasted image 20240208233301.png]]
![[Pasted image 20240208230428.png]]**Theorem 3.2.7** (*Valid PMFs*). Let $X$ be a discrete r.v. with support $x_1, x_2, \ldots$ 
The PMF $p_X$ of $X$ must satisfy the following two criteria:
- Nonnegative: $p_X(x)>0$ if $x=x_j$ for some $j$, and $p_X(x)=0$ otherwise;
- Sums to $1: \sum_{j=1}^{\infty} p_X\left(x_j\right)=1$.


# Slides notes

# Exercises
## SS.1

We have three coins (green, red, blue). The green is symmetrical, while the probability of tails is 0.3 for the red coin and 0.4 for the blue coin.

We perform the following experiment: We first toss the green coin. If we get heads, we throw then the red coin; otherwise we toss the blue coin. We register at the end whether we got a tails or heads in the second toss, which was therefore either with the red or the blue coin.

### 1
Explain the following conditional probabilities

$$
\begin{gathered}
P(\text { tails } \mid \text { red })=0.3 \\
P(\text { tails } \mid \text { blue })=0.4 \\
P(\text { red })=P(\text { blue })=0.5
\end{gathered}
$$

The *probability* of*event* tails *conditioned* on red is $0.3$

The *probability* of *event* tails conditioned on blue is 0.4

The *probability* of *event* red or blue is 0.5, since it is decided after we've tossed the green coin which is *symmetrical*
### 2
Decide $P(\text{tails})$ by using LOTP
![[2024.02.09#LOTP]]

$$
\begin{aligned}
P(\text{tails}) & = P(T\mid R)P(R)+P(T\mid B)P(B) \\
 & = 0.3 \cdot 0.5+0.4\cdot0.5 \\
 & = 0.35
\end{aligned}
$$ 
### 3
Decide $P(R\mid \text{tails})$ by using Bayes' rule
![[2024.02.09#Bayes' rule]]


## SS.4
Look at R files

## BHO 1.56
### a
Binomial coefficient 10/5=252

$\frac{10\cdot9\cdot8\cdot7\cdot6}{5!}=252$

Binomial coefficient 10/6=210

### b
Binomial coefficient (10/5)times0.5=126

### c
$(\frac{1}{365})^3$

