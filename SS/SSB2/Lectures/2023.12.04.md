# Lecture notes

## Mean value of discrete stochastic variables
Let $X: S \rightarrow \mathbb{R}$ be a *discrete stochastic variable* with support $C=\left\{x_1, \ldots, x_n\right\} \subset \mathbb{R}$ and probability function $p_X: \mathbb{R} \rightarrow[0,1]$.

The mean value of $X$ is given with:

$$
\begin{aligned} \mathrm{E}[X] & =\sum_{j=1}^n x_j \cdot p_X\left(x_j\right) \\ & =x_1 \cdot p_X\left(x_1\right)+\ldots+x_n \cdot p_X\left(x_n\right)\end{aligned}
$$

## Linearity of mean value
**Theorem 4.2.1**
Let $X, Y: S \rightarrow \mathbb{R}$ be *stochastic variables* and let $a, b \in \mathbb{R}$:
$$
\mathrm{E}[X+Y]=\mathrm{E}[X]+\mathrm{E}[Y], \quad \mathrm{E}[a+b \cdot X]=a+b \cdot \mathrm{E}[X]
$$

## Law of the unconscious statistician (LOTUS)

Let $X$ be a discrete *stochastic variable* with *support* $C$ and the probability function $p_X$.

Let $g:\mathbb{R} \rightarrow \mathbb{R}$ be a function. We can find the mean value of the transformed stochastic variable $Y=g(X)$ using *LOTUS*:
$$
\mathrm{E}[g(X)]=\sum_{x \in C} g(x) \cdot P(X=x)=\sum_{x \in C} g(x) \cdot p_X(x)
$$
## Variance, Standard Deviation
**Definition 4.6.1**: *Variance* and *Standard Deviation* are defined as:
$$
\operatorname{Var}(X)=\mathrm{E}\left[(X-\mathrm{E}[X])^2\right], \quad \quad \mathrm{SD}(X)=\sqrt{\operatorname{Var}(x)}
$$
### Math rules for variance

Let $X, Y$ be *stochastic variables* and $a, b \in \mathbb{R}$ then:

- $\operatorname{Var}(a+b \cdot X)=b^2 \cdot \operatorname{Var}(X)$, and thus $\operatorname{SD}(a+b \cdot X)=|b| \cdot \operatorname{SD}(X)$
- $\operatorname{Var}(X) \geq 0$, with an equal sign if and only if $a \in \mathbb{R}$ such that $P(X=a)=1$
- If $X$ and $Y$ are *independent* then $\operatorname{Var}(X+Y)=\operatorname{Var}(X)+\operatorname{Var}(Y)$


**Theorem 4.6.2**:
$$
\operatorname{Var}(X)=\mathrm{E}\left[X^2\right]-(\mathrm{E}[X])^2
$$
#### Proof
$$
\begin{aligned}
\operatorname{Var}(X) & =\mathrm{E}\left[(X-\mathrm{E}[X])^2\right] \\
& =\mathrm{E}\left[X^2-2 \cdot \mathrm{E}[X] \cdot X+(\mathrm{E}[X])^2\right] \\
& =\mathrm{E}\left[X^2\right]-2 \cdot \mathrm{E}[X] \cdot \mathrm{E}[X]+(\mathrm{E}[X])^2 \\
& =\mathrm{E}\left[X^2\right]-(\mathrm{E}[X])^2
\end{aligned}
$$


## Variance in Bernoulli and binomial
Let $X \sim \operatorname{Bern}(p)$, then
$$
\mathrm{E}[X]=p, \quad \operatorname{Var}(X)=p \cdot(1-p), \quad \operatorname{SD}(X)=\sqrt{p \cdot(1-p)}
$$
**Theorem 4.6.5**
Let $X \sim \operatorname{Bin}(n, p)$, then
$$
\mathrm{E}[X]=n \cdot p, \quad \operatorname{Var}(X)=n \cdot p \cdot(1-p), \quad \operatorname{SD}(X)=\sqrt{n \cdot p \cdot(1-p)}
$$
## Continuous distributions
### Example: target practice
![[Pasted image 20231204110648.png]]
The *probability space* $S$ with probability goal $P$:
- $S$ is the *unit circle*, such that $S=\left\{(u, v) \in \mathbb{R}^2 \mid u^2+v^2 \leq 1\right\}$.
- $P$ is the *continuous distribution*, such that there is the same probability of hitting anywhere on the circle.

Possible values for $X$ are $[0,1]$.

Same probability of hitting anywhere on the target. Probability of hitting a point is proportional to the size of the target!

This gives us the density function:
$$
F(x)=P(X \leq x)= \begin{cases}0 & \text { if } x<0 \\ x^2 & \text { if } 0 \leq x \leq 1 \\ 1 & \text { if } x>1\end{cases}
$$

## Continuous stochastic variables
**Definition 5.1.1** $X$ is a *continuous stochastic variable* if its density function is *differentiable* at most in a finite amount of points where the density function is *continuous*.

Point probabilities and probabilities for intervals:
- Where $F$ is continuous $x \in \mathbb{R}: P(X=x)=0~\forall x \in \mathbb{R}$.
- For $a<b$ it holds that
$$
P(a<X \leq b)=P(X \in(a, b])=F(b)-F(a)
$$

## Probability density function
$X$ is a *continuous stochastic variable* with density function $F$.

**Definition 5.1.2**: Probability Density Funcion for $X$ is given with
$$
f(x)= \begin{cases}F^{\prime}(x) & \text { if } F \text { is differentiable } \mathrm{in } ~x \\ 0 & \text { else }\end{cases}
$$

The support for $X$ is defined with $M=\{x \in \mathbb{R}: f(x)>0\}$.

We say that $X$ is concentrated on $M$.


**Theorem 5.1.3**

### Probability as the area under the density function
![[Pasted image 20231204113053.png]]


Probability for interval $=$ area under the curve:
$$
P(a<X \leq b)=\int_a^b f(x) \mathrm{d} x
$$

### Construction of density/distribution functions


**Theorem 5.1.5**: Egenskaber ved tæthedsfunktioner $f: \mathbb{R} \rightarrow[0, \infty)$ er en tæthedsfunktion hvis og kun hvis:
- Ikke negativ: $f(x) \geq 0$
- Integrerer til 1: $\int_{-\infty}^{\infty} f(x) \mathrm{d} x=1$

**Theorem 3.6.3**: Egenskaber ved fordelingsfunktioner
$F: \mathbb{R} \rightarrow[0,1]$ er en fordelingsfunktion hvis og kun hvis:
- Svagt voksende: Hvis $x \leq y$, så $F(x) \leq F(y)$
- Højrekontinuert: $F(x)=\lim _{y \downarrow x} F(y)=\inf _{y>x} F(y)$
- Opfylder $\lim _{x \rightarrow-\infty} F(x)=0$ og $\lim _{x \rightarrow \infty} F(x)=1$

### Exponential distribution